{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Basics of Gradient Descent\n",
    "\n",
    "Gradient descent is an optimization algorithm that attempts to find the minimum of a function.\n",
    "\n",
    "As the name implies, the algorithm makes use of a function's gradient.\n",
    "\n",
    "For some function $f(x, y, z, ...)$, we'll use its gradient ,$\\nabla f$, to pick successive points closer and closer to a local minimum from some initial guess $(x_{0}, y_{0}, z_{0}, ...)$\n",
    "\n",
    "\n",
    "Let's start with a simple one-dimensional example.\n",
    "\n",
    "Optimize (Find the minimum value of):\n",
    "$$f(x) = 7x^2 - 17x - 193$$\n",
    "\n",
    "![image.png](http://localhost:8888/tree/Desktop/Python/Parabola.png)\n",
    "\n",
    "It's a parabola, with the minimum located somewhere between 1 and 2.\n",
    "\n",
    "Let's start with an initial guess of $x_{0} = 15$.\n",
    "![image.png](http://localhost:8888/tree/Desktop/Python/Initial Guess.png)\n",
    "\n",
    "We need to move our guess to the left of our initial pick to get to the minimum value of the function.\n",
    "\n",
    "But how can we automate this, make it \"computer friendly?\"\n",
    "\n",
    "We can start by trying to figure out which way is \"downhill\". \n",
    "\n",
    "$x_{n+1} = x_{n} - \\alpha \\nabla f(x_{n})$ \n",
    "\n",
    "In order to start with gradient descent, we'll need the gradient. For functions that only have one variable, this is equivalent to taking the derivative. \n",
    "\n",
    "$$\\nabla f(x) = \\frac {d}{dx} f(x) = 14x - 17$$\n",
    "\n",
    "Since the derivative tells us the direction the graph is increasing in, we'll need to adjust our guess in the other direction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to minimize - f(x) = 7x^2 - 17x - 193\n",
    "# f'(x) = 14x - 17\n",
    "\n",
    "initial_guess = 1500\n",
    "learning_parameter = 0.1\n",
    "\n",
    "def gradient_descent(starting_point, learning_rate, iterations):\n",
    "    for i in range(iterations):\n",
    "        starting_point = starting_point - learning_parameter * (14 * starting_point - 17)\n",
    "    return starting_point\n",
    "\n",
    "\n",
    "for i in (1, 2, 3, 4, 5, 10, 100, 100000):\n",
    "    print(gradient_descent(initial_guess, learning_parameter, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36*x + 50000/x**6\n"
     ]
    }
   ],
   "source": [
    "# lets try to generalize the algorithm\n",
    "\n",
    "import sympy as sympy\n",
    "\n",
    "x = sympy.Symbol('x')\n",
    "\n",
    "minimize_this_function = 18*x**2 - 10000*x**-5 + 9078\n",
    "print(sympy.diff(minimize_this_function, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
